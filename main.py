"""
GraphRAG Pipeline - Main Orchestrator
Káº¿t há»£p táº¥t cáº£ cÃ¡c bÆ°á»›c Ä‘á»ƒ táº¡o thÃ nh pipeline hoÃ n chá»‰nh
"""

import os
import time
from typing import List
from dotenv import load_dotenv

# Import cÃ¡c bÆ°á»›c
from step1_chunking import chunk_documents
from step2_extraction import EntityRelationExtractor
from step3_graph_building import GraphBuilder
from step4_community_detection import CommunityDetector
from step5_answer_generation import AnswerGenerator
from docx_reader import read_docx_from_directory


class GraphRAGPipeline:
    """
    Pipeline hoÃ n chá»‰nh cho GraphRAG
    
    Flow:
    Documents â†’ Chunks â†’ Extractions â†’ Graph â†’ Communities â†’ Answer
    """
    
    def __init__(self, api_key: str):
        """
        Khá»Ÿi táº¡o pipeline
        
        Args:
            api_key: OpenAI API key
        """
        self.api_key = api_key
        self.extractor = EntityRelationExtractor(api_key)
        self.graph_builder = GraphBuilder()
        self.community_detector = CommunityDetector()
        self.answer_generator = AnswerGenerator(api_key)
    
    def run(self, documents: List[str], query: str) -> str:
        """
        Cháº¡y toÃ n bá»™ pipeline
        
        Args:
            documents: List of document texts
            query: User's question
            
        Returns:
            str: Final answer
        """
        start_time = time.time()
        
        print("\n" + "=" * 80)
        print("ğŸš€ GRAPHRAG PIPELINE - Báº®T Äáº¦U")
        print("=" * 80)
        print(f"ğŸ“š Sá»‘ documents: {len(documents)}")
        print(f"ğŸ“ Tá»•ng sá»‘ kÃ½ tá»±: {sum(len(d) for d in documents):,}")
        print(f"â“ Query: {query}")
        print("=" * 80)
        
        # ==================== STEP 1: CHUNKING ====================
        print("\n[STEP 1/5] Chunking documents...")
        step1_output = chunk_documents(documents)
        step1_output.print_summary()
        step1_output.save_to_file()
        
        # ==================== STEP 2: EXTRACTION ====================
        print("\n[STEP 2/5] Extracting entities & relations...")
        step2_output = self.extractor.extract(step1_output.chunks)
        step2_output.print_summary()
        step2_output.save_to_file()
        
        # ==================== STEP 3: GRAPH BUILDING ====================
        print("\n[STEP 3/5] Building knowledge graph...")
        step3_output = self.graph_builder.build(step2_output.extractions)
        step3_output.print_summary()
        step3_output.save_to_file()
        
        # ==================== STEP 4: COMMUNITY DETECTION ====================
        print("\n[STEP 4/5] Detecting communities...")
        step4_output = self.community_detector.detect(step3_output.graph)
        step4_output.print_summary()
        step4_output.save_to_file()
        
        # ==================== STEP 5: ANSWER GENERATION ====================
        print("\n[STEP 5/5] Generating answer...")
        step5_output = self.answer_generator.generate(
            step4_output.communities,
            query
        )
        step5_output.print_summary()
        step5_output.save_to_file()
        
        # ==================== SUMMARY ====================
        elapsed = time.time() - start_time
        print("\n" + "=" * 80)
        print("âœ… PIPELINE HOÃ€N THÃ€NH")
        print("=" * 80)
        print(f"â±ï¸  Tá»•ng thá»i gian: {elapsed:.2f}s")
        print(f"ğŸ“Š Pipeline stats:")
        print(f"   - Documents â†’ Chunks: {len(documents)} â†’ {step1_output.stats['num_chunks']}")
        print(f"   - Chunks â†’ Extractions: {step1_output.stats['num_chunks']} â†’ {step2_output.stats['num_extractions']}")
        print(f"   - Extractions â†’ Graph: {step2_output.stats['num_extractions']} â†’ {step3_output.stats['num_nodes']} nodes, {step3_output.stats['num_edges']} edges")
        print(f"   - Graph â†’ Communities: {step3_output.stats['num_nodes']} nodes â†’ {step4_output.stats['large_communities']} communities")
        print(f"   - Communities â†’ Answer: {step4_output.stats['large_communities']} communities â†’ 1 answer")
        print("=" * 80)
        print(f"\nğŸ“ Táº¥t cáº£ output files Ä‘Ã£ Ä‘Æ°á»£c lÆ°u trong thÆ° má»¥c: pipeline_outputs/")
        
        return step5_output.answer


def main():
    """Main entry point"""
    # Load environment
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")
    
    if not api_key:
        print("âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y OPENAI_API_KEY trong file .env")
        return
    
    # Load documents
    print("\nğŸ“‚ Äang load documents tá»« thÆ° má»¥c 'example_docx'...")
    documents = read_docx_from_directory("example_docx")
    
    if not documents:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y file .docx trong thÆ° má»¥c 'example_docx'")
        return
    
    print(f"âœ… ÄÃ£ load {len(documents)} documents")
    
    # Get query from user
    query = input("\nâ“ Nháº­p cÃ¢u há»i cá»§a báº¡n: ").strip()
    if not query:
        query = "Tá»•ng há»£p ná»™i dung chÃ­nh cá»§a cÃ¡c tÃ i liá»‡u"
        print(f"   (Sá»­ dá»¥ng query máº·c Ä‘á»‹nh: {query})")
    
    # Run pipeline
    pipeline = GraphRAGPipeline(api_key)
    answer = pipeline.run(documents, query)
    
    print("\nğŸ‰ HoÃ n thÃ nh!")


if __name__ == "__main__":
    main()