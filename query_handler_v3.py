# query_handler_v3.py
"""
ENHANCED Query Handler V3
- Improved Vietnamese query processing
- Better fallback mechanisms
- Context-aware responses
- Enhanced entity matching
"""

from graph_manager_v3 import GraphManagerV3
from logger import Logger
from typing import List, Dict, Optional
import re
import unicodedata


class QueryHandlerV3:
    """
    Enhanced Query Handler with:
    - Smart query term extraction
    - Multiple search strategies
    - Intelligent fallback responses
    - Vietnamese text normalization
    """

    logger = Logger("QueryHandlerV3").get_logger()

    def __init__(self, graph_manager: GraphManagerV3, client, model: str):
        self.graph_manager = graph_manager
        self.client = client
        self.model = model
        self.logger.info(f"Initialized QueryHandlerV3 with model={model}")

    # =========================================================
    # MAIN QUERY HANDLER
    # =========================================================
    
    def ask_question(
        self,
        query: str,
        k: int = 2,
        top_k_seeds: int = 5,
        max_nodes: int = 80,
        use_embeddings: bool = True
    ) -> str:
        """
        Answer question using k-hop retrieval with intelligent fallback.
        
        Args:
            query: User question
            k: K-hop depth (1-3)
            top_k_seeds: Number of seed entities (3-10)
            max_nodes: Max nodes in subgraph (50-200)
            use_embeddings: Use semantic search
            
        Returns:
            Answer string
        """
        # Normalize query
        query = unicodedata.normalize('NFC', query)
        
        self.logger.info(f"Processing query: {query}")
        self.logger.info(f"  Settings: k={k}, seeds={top_k_seeds}, max_nodes={max_nodes}, embeddings={use_embeddings}")

        # Step 1: Extract query terms and entities
        query_analysis = self._analyze_query(query)
        
        self.logger.info(f"  Query analysis:")
        self.logger.info(f"    - Terms: {query_analysis['terms']}")
        self.logger.info(f"    - Entities: {query_analysis['entities']}")
        self.logger.info(f"    - Question type: {query_analysis['question_type']}")
        
        # Step 2: Find seed entities
        seed_entities = self.graph_manager.find_relevant_entities(
            query_terms=query_analysis['all_keywords'],
            top_k=top_k_seeds,
            use_embeddings=use_embeddings
        )

        # FALLBACK 1: No entities found
        if not seed_entities:
            return self._generate_no_entities_response(query, query_analysis)

        self.logger.info(f"  Found {len(seed_entities)} seed entities: {seed_entities}")

        # Step 3: Get subgraph
        subgraph = self.graph_manager.get_k_hop_subgraph(
            seed_entities=seed_entities,
            k=k,
            max_nodes=max_nodes
        )

        # FALLBACK 2: Empty subgraph
        if not subgraph or not subgraph.get('nodes'):
            return self._generate_empty_subgraph_response(query, seed_entities)

        # Step 4: Check subgraph relevance
        relevance_check = self._check_subgraph_relevance(
            subgraph, query, query_analysis
        )
        
        self.logger.info(f"  Relevance check: {relevance_check['has_relevant_data']}")
        if relevance_check['missing_info']:
            self.logger.info(f"    Missing: {relevance_check['missing_info']}")

        # Step 5: Format context
        context = self.graph_manager.format_subgraph_for_context(subgraph)

        # Step 6: Generate response with LLM
        system_prompt = self._build_system_prompt(
            query_analysis,
            relevance_check
        )
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {
                    "role": "system",
                    "content": system_prompt
                },
                {
                    "role": "user",
                    "content": (
                        f"CÃ¢u há»i: {query}\n\n"
                        f"Seed entities: {', '.join(seed_entities)}\n\n"
                        f"Knowledge graph context:\n{context}\n\n"
                        f"HÃ£y tráº£ lá»i chÃ­nh xÃ¡c dá»±a trÃªn thÃ´ng tin trong graph."
                    )
                }
            ],
            max_tokens=1000,
            temperature=0
        )

        answer = response.choices[0].message.content

        # Step 7: Enhance answer if needed
        if relevance_check['missing_info']:
            answer = self._enhance_answer_with_missing_info(
                answer,
                relevance_check['missing_info']
            )

        return answer

    # =========================================================
    # QUERY ANALYSIS
    # =========================================================
    
    def _analyze_query(self, query: str) -> Dict:
        """
        Analyze query to extract:
        - Main entities mentioned
        - Query terms
        - Question type
        """
        query_lower = query.lower()
        
        # Detect question type
        question_type = self._detect_question_type(query_lower)
        
        # Extract entities mentioned in query
        entities = self._extract_mentioned_entities(query)
        
        # Extract query terms
        terms = self._extract_query_terms(query)
        
        # Combine all keywords
        all_keywords = list(set(entities + terms))
        
        return {
            'question_type': question_type,
            'entities': entities,
            'terms': terms,
            'all_keywords': all_keywords
        }
    
    def _detect_question_type(self, query_lower: str) -> str:
        """Detect what kind of question is being asked."""
        
        patterns = {
            'instructor': ['giáº£ng viÃªn', 'giÃ¡o viÃªn', 'tháº§y', 'cÃ´', 'ai giáº£ng', 'ai dáº¡y'],
            'email': ['email', 'mail', 'liÃªn há»‡'],
            'credits': ['tÃ­n chá»‰', 'sá»‘ tÃ­n', 'credit'],
            'code': ['mÃ£ há»c pháº§n', 'mÃ£ mÃ´n', 'code'],
            'prerequisite': ['tiÃªn quyáº¿t', 'Ä‘iá»u kiá»‡n', 'prerequisite', 'há»c trÆ°á»›c'],
            'materials': ['tÃ i liá»‡u', 'sÃ¡ch', 'giÃ¡o trÃ¬nh', 'reference'],
            'software': ['pháº§n má»m', 'software', 'cÃ´ng cá»¥', 'tool'],
            'objectives': ['má»¥c tiÃªu', 'objective', 'goal'],
            'outcomes': ['chuáº©n Ä‘áº§u ra', 'clo', 'outcome', 'learning outcome'],
            'description': ['mÃ´ táº£', 'description', 'ná»™i dung', 'vá» gÃ¬'],
            'assessment': ['Ä‘Ã¡nh giÃ¡', 'assessment', 'thi', 'kiá»ƒm tra'],
            'hours': ['giá»', 'hour', 'thá»i gian'],
            'department': ['khoa', 'viá»‡n', 'department', 'faculty']
        }
        
        for q_type, keywords in patterns.items():
            if any(kw in query_lower for kw in keywords):
                return q_type
        
        return 'general'
    
    def _extract_mentioned_entities(self, query: str) -> List[str]:
        """
        Extract entity names mentioned in query.
        Enhanced for Vietnamese course/instructor names.
        """
        entities = []
        
        # Method 1: Look for quoted strings (explicit mentions)
        quoted = re.findall(r'"([^"]+)"', query)
        entities.extend(quoted)
        
        # Method 2: Look for capitalized phrases
        # Vietnamese course names often have caps
        words = query.split()
        current_entity = []
        
        for word in words:
            # Check if word starts with capital or is all caps
            # But exclude common question words even if capitalized
            if word and (word[0].isupper() or word.isupper()):
                # Skip if it's a question word
                if word.lower() not in ['gÃ¬', 'nÃ o', 'ai', 'Ä‘Ã¢u', 'sao']:
                    current_entity.append(word)
            else:
                if current_entity:
                    entity_name = ' '.join(current_entity)
                    if len(entity_name) > 3:  # Filter out short caps
                        entities.append(entity_name)
                    current_entity = []
        
        # Add last entity
        if current_entity:
            entity_name = ' '.join(current_entity)
            if len(entity_name) > 3:
                entities.append(entity_name)
        
        # Method 3: Look for common Vietnamese name patterns
        # Teacher names: "ThS. ...", "TS. ...", "PGS. ...", "GS. ..."
        name_patterns = [
            r'((?:ThS|TS|PGS|GS)\.?\s+[A-ZÄÄ‚Ã‚ÃŠÃ”Æ Æ¯][a-zÄ‘ÄƒÃ¢ÃªÃ´Æ¡Æ°]+(?:\s+[A-ZÄÄ‚Ã‚ÃŠÃ”Æ Æ¯][a-zÄ‘ÄƒÃ¢ÃªÃ´Æ¡Æ°]+)+)',
            r'((?:Tháº¡c sÄ©|Tiáº¿n sÄ©|PhÃ³ GiÃ¡o sÆ°|GiÃ¡o sÆ°)\s+[A-ZÄÄ‚Ã‚ÃŠÃ”Æ Æ¯][a-zÄ‘ÄƒÃ¢ÃªÃ´Æ¡Æ°]+(?:\s+[A-ZÄÄ‚Ã‚ÃŠÃ”Æ Æ¯][a-zÄ‘ÄƒÃ¢ÃªÃ´Æ¡Æ°]+)+)'
        ]
        
        for pattern in name_patterns:
            matches = re.findall(pattern, query)
            entities.extend(matches)
        
        # Deduplicate
        entities = list(dict.fromkeys(entities))  # Preserve order
        
        return entities
    
    def _extract_query_terms(self, query: str) -> List[str]:
        """
        Extract meaningful terms from query.
        Filter out stopwords and common question words.
        """
        # Normalize
        query = unicodedata.normalize('NFC', query)
        
        # Extract words
        terms = re.findall(r'\w+', query.lower())

        # Vietnamese stopwords - MINIMAL set
        # Only remove truly meaningless words, keep domain-specific terms
        stopwords = {
            # Question words
            'lÃ ', 'bao', 'nhiÃªu', 'cÃ³', 'máº¥y', 'gÃ¬', 'nÃ o', 'tháº¿', 
            'ai', 'khi', 'nÃ o', 'Ä‘Ã¢u', 'sao', 'thÃ¬', 'nÃ y', 'Ä‘Ã³',
            
            # Conjunctions & prepositions
            'vÃ ', 'vá»›i', 'trong', 'vá»', 'cho', 'cá»§a', 'Ä‘á»ƒ', 'tá»«', 
            'hay', 'hoáº·c', 'nhÆ°ng', 'mÃ ',
            
            # Verb helpers
            'khÃ´ng', 'chÆ°a', 'Ä‘Ã£', 'sáº½', 'Ä‘ang', 'váº«n', 'Ä‘Æ°á»£c',
            
            # Articles/determiners  
            'cÃ¡c', 'nhá»¯ng', 'má»™t', 'cÃ¡i',
            
            # Common but meaningless
            'nhÆ°', 'em', 'tÃ´i', 'báº¡n', 'áº¡'
        }
        
        # NOTE: We do NOT remove domain terms like:
        # - 'mÃ´n', 'há»c', 'pháº§n' (course-related)
        # - 'cho' when part of entity name
        # These may be part of entity names or important context

        # Filter
        meaningful_terms = [
            t for t in terms 
            if t not in stopwords and len(t) > 2
        ]
        
        return meaningful_terms

    # =========================================================
    # RELEVANCE CHECKING
    # =========================================================
    
    def _check_subgraph_relevance(
        self,
        subgraph: Dict,
        query: str,
        query_analysis: Dict
    ) -> Dict:
        """
        Check if subgraph contains relevant information to answer query.
        
        Returns:
            {
                'has_relevant_data': bool,
                'missing_info': List[str],
                'found_info': List[str]
            }
        """
        question_type = query_analysis['question_type']
        nodes = subgraph.get('nodes', [])
        edges = subgraph.get('edges', [])
        
        found_info = []
        missing_info = []
        
        # Check based on question type
        if question_type == 'instructor':
            has_instructor = any(n.get('type') == 'giáº£ng_viÃªn' for n in nodes)
            has_teaching_rel = any('GIáº¢NG' in e.get('type', '').upper() for e in edges)
            
            if has_instructor or has_teaching_rel:
                found_info.append('thÃ´ng tin giáº£ng viÃªn')
            else:
                missing_info.append('thÃ´ng tin giáº£ng viÃªn')
        
        elif question_type == 'email':
            has_email = any(n.get('email') for n in nodes)
            
            if has_email:
                found_info.append('email')
            else:
                missing_info.append('email')
        
        elif question_type == 'credits':
            has_credits = any(
                n.get('sá»‘_tÃ­n_chá»‰') or n.get('tÃ­n_chá»‰')
                for n in nodes
            )
            
            if has_credits:
                found_info.append('sá»‘ tÃ­n chá»‰')
            else:
                missing_info.append('sá»‘ tÃ­n chá»‰')
        
        elif question_type == 'code':
            has_code = any(n.get('mÃ£_há»c_pháº§n') for n in nodes)
            
            if has_code:
                found_info.append('mÃ£ há»c pháº§n')
            else:
                missing_info.append('mÃ£ há»c pháº§n')
        
        elif question_type == 'prerequisite':
            has_prereq = any('TIÃŠN_QUYáº¾T' in e.get('type', '').upper() for e in edges)
            
            if has_prereq:
                found_info.append('há»c pháº§n tiÃªn quyáº¿t')
            else:
                missing_info.append('há»c pháº§n tiÃªn quyáº¿t')
        
        elif question_type == 'materials':
            has_materials = any(n.get('type') == 'tÃ i_liá»‡u' for n in nodes)
            
            if has_materials:
                found_info.append('tÃ i liá»‡u')
            else:
                missing_info.append('tÃ i liá»‡u')
        
        elif question_type == 'software':
            has_software = any(n.get('type') == 'pháº§n_má»m' for n in nodes)
            
            if has_software:
                found_info.append('pháº§n má»m')
            else:
                missing_info.append('pháº§n má»m')
        
        has_relevant_data = len(found_info) > 0
        
        return {
            'has_relevant_data': has_relevant_data,
            'found_info': found_info,
            'missing_info': missing_info
        }

    # =========================================================
    # SYSTEM PROMPT BUILDING
    # =========================================================
    
    def _build_system_prompt(
        self,
        query_analysis: Dict,
        relevance_check: Dict
    ) -> str:
        """Build context-aware system prompt."""
        
        question_type = query_analysis['question_type']
        has_data = relevance_check['has_relevant_data']
        missing_info = relevance_check['missing_info']
        
        base_prompt = """Báº¡n lÃ  há»‡ thá»‘ng tráº£ lá»i cÃ¢u há»i vá» chÆ°Æ¡ng trÃ¬nh Ä‘Ã o táº¡o dá»±a trÃªn Knowledge Graph.

QUY Táº®C CHUNG:
- CHá»ˆ tráº£ lá»i dá»±a trÃªn thÃ´ng tin cÃ³ trong graph context
- Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t tá»± nhiÃªn, rÃµ rÃ ng
- KHÃ”NG suy Ä‘oÃ¡n hoáº·c bá»‹a thÃ´ng tin khÃ´ng cÃ³ trong graph
- Náº¿u thiáº¿u thÃ´ng tin, nÃªu rÃµ pháº§n nÃ o THIáº¾U
"""
        
        # Add question-type specific instructions
        if question_type == 'instructor':
            base_prompt += """
LOáº I CÃ‚U Há»ŽI: Giáº£ng viÃªn
- Liá»‡t kÃª Táº¤T Cáº¢ giáº£ng viÃªn tÃ¬m tháº¥y
- Bao gá»“m email náº¿u cÃ³
- Bao gá»“m chá»©c danh náº¿u cÃ³
"""
        
        elif question_type == 'email':
            base_prompt += """
LOáº I CÃ‚U Há»ŽI: Email/LiÃªn há»‡
- Cung cáº¥p email chÃ­nh xÃ¡c
- Náº¿u cÃ³ nhiá»u ngÆ°á»i, liá»‡t kÃª táº¥t cáº£
"""
        
        elif question_type == 'credits' or question_type == 'code':
            base_prompt += """
LOáº I CÃ‚U Há»ŽI: ThÃ´ng tin há»c pháº§n
- Tráº£ lá»i chÃ­nh xÃ¡c sá»‘ liá»‡u
- NÃªu rÃµ Ä‘Æ¡n vá»‹ (tÃ­n chá»‰, giá», v.v.)
"""
        
        elif question_type == 'materials':
            base_prompt += """
LOáº I CÃ‚U Há»ŽI: TÃ i liá»‡u tham kháº£o
- Liá»‡t kÃª Ä‘áº§y Ä‘á»§ tÃ i liá»‡u
- PhÃ¢n loáº¡i: GiÃ¡o trÃ¬nh / TÃ i liá»‡u tham kháº£o
- Bao gá»“m tÃ¡c giáº£, nÄƒm xuáº¥t báº£n náº¿u cÃ³
"""
        
        # Add data availability notice
        if not has_data and missing_info:
            base_prompt += f"""
âš ï¸ CHÃš Ã: Graph THIáº¾U thÃ´ng tin vá»: {', '.join(missing_info)}

Khi tráº£ lá»i:
1. NÃªu rÃµ thÃ´ng tin NÃ€O cÃ³ trong graph
2. NÃªu rÃµ thÃ´ng tin NÃ€O thiáº¿u
3. Äá» xuáº¥t cÃ¡ch tÃ¬m thÃ´ng tin thiáº¿u (há»i cá»¥ thá»ƒ hÆ¡n, hoáº·c liÃªn há»‡ khoa)
"""
        else:
            base_prompt += """
- TrÃ¬nh bÃ y thÃ´ng tin Ä‘áº§y Ä‘á»§ vÃ  cÃ³ tá»• chá»©c
- Sá»­ dá»¥ng bullet points khi cáº§n thiáº¿t
"""
        
        return base_prompt

    # =========================================================
    # FALLBACK RESPONSES
    # =========================================================
    
    def _generate_no_entities_response(
        self,
        query: str,
        query_analysis: Dict
    ) -> str:
        """Generate response when no entities found."""
        
        self.logger.warning(f"No entities found for query: {query}")
        
        # Try to suggest similar entities
        suggestions = []
        for term in query_analysis['all_keywords']:
            similar = self.graph_manager.search_entities(term, limit=3)
            suggestions.extend(similar)
        
        suggestions = list(set(suggestions))[:5]
        
        response = "âš ï¸ KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin phÃ¹ há»£p trong cÆ¡ sá»Ÿ dá»¯ liá»‡u.\n\n"
        
        if suggestions:
            response += "CÃ³ thá»ƒ báº¡n Ä‘ang tÃ¬m kiáº¿m:\n"
            for s in suggestions:
                response += f"  â€¢ {s}\n"
            response += "\nðŸ’¡ HÃ£y thá»­ há»i láº¡i vá»›i cÃ¡c tÃªn nÃ y."
        else:
            response += "ðŸ’¡ Gá»£i Ã½:\n"
            response += "  â€¢ Há»i vá» cÃ¡c há»c pháº§n cá»¥ thá»ƒ (vÃ­ dá»¥: 'PhÃ¢n tÃ­ch vÃ  thiáº¿t káº¿ há»‡ thá»‘ng')\n"
            response += "  â€¢ Há»i vá» giáº£ng viÃªn, tÃ i liá»‡u, hoáº·c chÆ°Æ¡ng trÃ¬nh Ä‘Ã o táº¡o\n"
            response += "  â€¢ Sá»­ dá»¥ng tÃªn Ä‘áº§y Ä‘á»§ cá»§a há»c pháº§n\n"
        
        return response
    
    def _generate_empty_subgraph_response(
        self,
        query: str,
        seed_entities: List[str]
    ) -> str:
        """Generate response when subgraph is empty."""
        
        self.logger.warning(f"Empty subgraph for entities: {seed_entities}")
        
        response = f"âœ“ TÃ¬m tháº¥y: {', '.join(seed_entities)}\n\n"
        response += "âš ï¸ Tuy nhiÃªn, khÃ´ng cÃ³ thÃ´ng tin bá»• sung hoáº·c má»‘i quan há»‡ nÃ o Ä‘Æ°á»£c ghi nháº­n.\n\n"
        response += "Äiá»u nÃ y cÃ³ thá»ƒ do:\n"
        response += "  â€¢ ThÃ´ng tin chÆ°a Ä‘Æ°á»£c cáº­p nháº­t Ä‘áº§y Ä‘á»§ vÃ o há»‡ thá»‘ng\n"
        response += "  â€¢ Entity nÃ y chÆ°a cÃ³ liÃªn káº¿t vá»›i cÃ¡c thÃ´ng tin khÃ¡c\n"
        response += "  â€¢ Cáº§n má»Ÿ rá»™ng tÃ¬m kiáº¿m (tÄƒng k-hop hoáº·c sá»‘ seed entities)\n"
        
        return response
    
    def _enhance_answer_with_missing_info(
        self,
        answer: str,
        missing_info: List[str]
    ) -> str:
        """Enhance answer with notice about missing information."""
        
        # Don't modify if answer already mentions missing info
        if any(word in answer.lower() for word in ['khÃ´ng cÃ³', 'thiáº¿u', 'chÆ°a cÃ³']):
            return answer
        
        enhancement = "\n\n---\n"
        enhancement += f"â„¹ï¸ LÆ°u Ã½: ThÃ´ng tin vá» **{', '.join(missing_info)}** chÆ°a cÃ³ trong cÆ¡ sá»Ÿ dá»¯ liá»‡u.\n"
        enhancement += "Báº¡n cÃ³ thá»ƒ:\n"
        enhancement += "  â€¢ Há»i cá»¥ thá»ƒ hÆ¡n vá» thÃ´ng tin cÃ³ sáºµn\n"
        enhancement += "  â€¢ LiÃªn há»‡ khoa/viá»‡n quáº£n lÃ½ Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t\n"
        
        return answer + enhancement

    # =========================================================
    # COMPATIBILITY
    # =========================================================
    
    def ask_question_with_khop(self, query: str, **kwargs) -> str:
        """Alias for compatibility."""
        return self.ask_question(query, **kwargs)