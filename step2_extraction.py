"""
STEP 2: Entity & Relation Extraction
Input: List of text chunks
Output: List of extracted entities and relations
"""

from typing import List
from openai import OpenAI
from concurrent.futures import ThreadPoolExecutor, as_completed
import os
from config import MAX_WORKERS, USE_MINI_MODEL, EXTRACTION_MAX_TOKENS


class ExtractionOutput:
    """Output c·ªßa b∆∞·ªõc extraction"""
    def __init__(self, extractions: List[str], stats: dict):
        self.extractions = extractions
        self.stats = stats
    
    def print_summary(self):
        print("\n" + "=" * 80)
        print("STEP 2: EXTRACTION - OUTPUT")
        print("=" * 80)
        print(f"üì• S·ªë chunks ƒë·∫ßu v√†o: {self.stats['num_chunks']}")
        print(f"üì§ S·ªë extraction results: {self.stats['num_extractions']}")
        print(f"‚ö° Workers song song: {self.stats['max_workers']}")
        print(f"ü§ñ Model s·ª≠ d·ª•ng: {self.stats['model']}")
        print(f"‚è±Ô∏è  Th·ªùi gian x·ª≠ l√Ω: {self.stats['processing_time']:.2f}s")
        print(f"\nüìã Sample extraction result ƒë·∫ßu ti√™n:")
        print("-" * 80)
        print(self.extractions[0][:500] if self.extractions else "Kh√¥ng c√≥ k·∫øt qu·∫£")
        print("-" * 80)
        print("=" * 80)
    
    def save_to_file(self, output_dir: str = "pipeline_outputs"):
        """L∆∞u output ra file txt"""
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        filepath = os.path.join(output_dir, "step2_extraction_output.txt")
        with open(filepath, "w", encoding="utf-8") as f:
            f.write("=" * 80 + "\n")
            f.write("STEP 2: EXTRACTION - DETAILED OUTPUT\n")
            f.write("=" * 80 + "\n\n")
            
            # Stats
            f.write("üìä TH·ªêNG K√ä:\n")
            f.write(f"   - S·ªë chunks ƒë·∫ßu v√†o: {self.stats['num_chunks']}\n")
            f.write(f"   - S·ªë extraction results: {self.stats['num_extractions']}\n")
            f.write(f"   - Workers song song: {self.stats['max_workers']}\n")
            f.write(f"   - Model s·ª≠ d·ª•ng: {self.stats['model']}\n")
            f.write(f"   - Th·ªùi gian x·ª≠ l√Ω: {self.stats['processing_time']:.2f}s\n\n")
            
            # All extractions
            f.write("=" * 80 + "\n")
            f.write("üìù T·∫§T C·∫¢ ENTITIES & RELATIONS:\n")
            f.write("=" * 80 + "\n\n")
            
            for i, extraction in enumerate(self.extractions):
                f.write(f"--- EXTRACTION {i+1}/{len(self.extractions)} ---\n")
                f.write(extraction)
                f.write("\n\n" + "-" * 80 + "\n\n")
        
        print(f"üíæ ƒê√£ l∆∞u output v√†o: {filepath}")
        return filepath


class EntityRelationExtractor:
    """Class ƒë·ªÉ extract entities v√† relations t·ª´ text chunks"""
    
    SYSTEM_PROMPT = """
You are an information extraction system.

Extract ENTITIES and RELATIONSHIPS from the text.

STRICT FORMAT (no explanation, no markdown):

ENTITY: <entity name>
RELATION: <entity_1> -> <relation> -> <entity_2>

Rules:
- Use '->' exactly for relations
- Entity names: max 5 words
- Use Vietnamese if the text is Vietnamese
- Do NOT invent relations not present in text

Example:
ENTITY: H·ªá ƒëi·ªÅu h√†nh
ENTITY: Ti·∫øn tr√¨nh
RELATION: H·ªá ƒëi·ªÅu h√†nh -> qu·∫£n l√Ω -> Ti·∫øn tr√¨nh
"""
    
    def __init__(self, api_key: str):
        self.client = OpenAI(api_key=api_key)
        self.model = "gpt-4o-mini" if USE_MINI_MODEL else "gpt-4o"
    
    def _process_single_chunk(self, item_data):
        """X·ª≠ l√Ω m·ªôt chunk"""
        index, chunk = item_data
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": self.SYSTEM_PROMPT},
                    {"role": "user", "content": chunk[:1500]}
                ],
                max_tokens=EXTRACTION_MAX_TOKENS
            )
            return index, response.choices[0].message.content
        except Exception as e:
            print(f"[WARN] Chunk {index} failed: {e}")
            return index, ""
    
    def extract(self, chunks: List[str]) -> ExtractionOutput:
        """
        Extract entities v√† relations t·ª´ t·∫•t c·∫£ chunks song song
        
        Args:
            chunks: List of text chunks
            
        Returns:
            ExtractionOutput: Object ch·ª©a extraction results v√† stats
        """
        import time
        start_time = time.time()
        
        results = [None] * len(chunks)
        
        # Batch processing v·ªõi ThreadPoolExecutor
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = {
                executor.submit(self._process_single_chunk, (i, chunk)): i
                for i, chunk in enumerate(chunks)
            }
            
            for future in as_completed(futures):
                idx, result = future.result()
                results[idx] = result
        
        # L·ªçc k·∫øt qu·∫£ kh√¥ng r·ªóng
        extractions = [r for r in results if r]
        
        processing_time = time.time() - start_time
        
        stats = {
            'num_chunks': len(chunks),
            'num_extractions': len(extractions),
            'max_workers': MAX_WORKERS,
            'model': self.model,
            'processing_time': processing_time
        }
        
        return ExtractionOutput(extractions, stats)


if __name__ == "__main__":
    # Test extraction
    from dotenv import load_dotenv
    load_dotenv()
    
    test_chunks = [
        "H·ªá ƒëi·ªÅu h√†nh qu·∫£n l√Ω t√†i nguy√™n h·ªá th·ªëng. CPU th·ª±c thi c√°c ti·∫øn tr√¨nh.",
        "Python l√† ng√¥n ng·ªØ l·∫≠p tr√¨nh. Django l√† framework c·ªßa Python."
    ]
    
    extractor = EntityRelationExtractor(api_key=os.getenv("OPENAI_API_KEY"))
    output = extractor.extract(test_chunks)
    output.print_summary()